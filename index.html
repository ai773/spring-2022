<html>
<head>
  <meta charset="utf-8"/>
  <!-- Material Design fonts -->
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto:300,400,500,700">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/icon?family=Material+Icons">

  <!-- Bootstrap -->
  <!-- <link rel="stylesheet" type="text/css" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css"> -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  <!-- Bootstrap Material Design -->
  <link rel="stylesheet" type="text/css" href="dist/css/bootstrap-material-design.css">
  <link rel="stylesheet" type="text/css" href="dist/css/ripples.min.css">
  <style>
    body {
    	padding-top: 70px;
    	font-weight: normal;
    }
    th {
    	font-weight: bold;
    }
	th, td {
	    font-size: 14px;
	    padding: 10px;
	    border: solid 1px lightgrey;
	}
	.topic {

	}
	.reading {
		max-width: 450px;
	}
	.no-class {
		background-color: lightgrey;
	}
  </style>
  <title>AI733: Special Topics in Artificial Intelligence - Deep Learning and Real-world Applications | Spring 2022</title>
</head>
<body>
	

<div class="navbar navbar-default navbar-fixed-top">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">AI733</a>
    </div>
    <div class="navbar-collapse collapse navbar-responsive-collapse">
      <ul class="nav navbar-nav">
        <li class="active"><a href="index.html">Home</a></li>
        <li><a href="reading-response.html">Reading Response</a></li>
        <li><a href="topic-presentation.html">Topic Presentation</a></li>
        <!-- <li><a href="assignments.html">Assignments</a></li> -->
<!--         <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Assignments <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="#">Assignment 1</a></li>
            <li><a href="#">Assignment 2</a></li>
            <li><a href="#">Assignment 3</a></li>
            <li><a href="#">Assignment 4</a></li>
          </ul>
        </li> -->
        <!-- <li><a href="design-project.html">Design Project</a></li> -->
        <!-- <li><a href="projects/index.html">Project Gallery</a></li> -->
        <li><a href="class-logistics.html">Class Logistics</a></li>
      </ul>
    </div>
  </div>
</div>


<div class="container-fluid main">
	<div class="row">
		<div class="col-md-12">
          <div class="well">
            <div>SNU CSE Spring 2022</div>
            <h1> AI773 (4190.773.001) Special Topics in Artificial Intelligence - Deep Learning and Real-world Applications</h1>

            <p> Deep learning is now an integral part of daily systems and tools people use, and therefore no longer is a concern of only academic research. You will get the front-row experience on practical issues in research and development of deep learning systems from the leading experts and researchers. Major course activities include:</p>

            <ul>
              <li><strong>Reading Response</strong>: You'll read and discuss important papers and articles in the field. Each week, there will be 1-2 reading assignments, for which you'll write a short response to.</li>
              <li><strong>Topic Presentation</strong>: Once a semester, you'll lead the class by summarizing the readings, and spurring the in-class discussion. </li>
              <!-- <li><strong>Design Project</strong>: In a semester-long team project, you'll design, build, and test your own crowdsourcing / social computing system. If you have an ongoing research project that might benefit from having a crowdsourcing / social computing component, connecting to your research is encouraged. </li> -->
              <li><strong>In-class Activities</strong>: Each class will feature activities that will help you understand core concepts introduced in the course. </li>
            </ul>

          </div>
        <div id="source-button" class="btn btn-primary btn-xs" style="display: none;">&lt; &gt;</div></div>		
	</div>

	<div class="row">
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Course Staff</h3>
	            </div>
	            <div class="panel-body">
	              <strong>Instructor: </strong><br/>
                  &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://sangdooyun.github.io/">Prof. Sangdoo Yun</a><br/>
               <br/>
	              <!-- &nbsp;&nbsp;&nbsp;&nbsp;<em>Office Hours</em>: 4-5pm Tue/Thu @ N1-605<br/><br/> -->
	              <strong>TAs: </strong></br>
                  &nbsp;&nbsp;&nbsp;&nbsp; <a href="">Ïú†ÏäπÎ£° </a><br/>
                  &nbsp;&nbsp;&nbsp;&nbsp; <a href="">Í∞ïÎ¥âÍ∑† </a><br/><br/>
	              <!-- &nbsp;&nbsp;&nbsp;&nbsp;<em>Office Hours</em>: TBD <br/><br/> -->
                <strong>Staff Mailing List</strong>:<br/>&nbsp;&nbsp;&nbsp;&nbsp; TBD <br/>
                &nbsp;&nbsp;&nbsp;&nbsp; note: this is a group email address that includes the instructors and the TAs. 
                 <!-- <a href="mailto:cs492@kixlab.org">cs492@kixlab.org</a> -->
	            </div>
	          </div>         
	        </div>	     
        </div>         
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Time &amp; Location</h3>
	            </div>
	            <div class="panel-body">
	              <strong>When</strong>: 10:00am-12:30pm Fri<br/>
	              <strong>Where</strong>: Zoom (TBD)
	            </div>
	          </div>         
	        </div>        
        </div> 
    
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Links</h3>
	            </div>
	            <div class="panel-body">
	              <strong>Course Website</strong>: <a href="https://ai773.github.io/spring-2022/">https://ai773.github.io/spring-2022/</a><br/>
	              <strong>Submission &amp; Grading</strong>: <a href="">ETL</a><br/>
	              <strong>Discussion Forum</strong>: TBD <a href=""></a>
	            </div>
	          </div>         
	        </div>        
        </div>
    </div>

	<div class="row">
		<div class="col-md-12">
	        <div class="bs-component">
	          <div class="panel panel-info">
	            <div class="panel-heading">
	              <h3 class="panel-title">Updates</h3>
	            </div>
	            <div class="panel-body">
	              <ul>
                  <li><p style="color:red"><strong>
                    3/3: You may "audit" or "sit in" this class, but you still have to submit reading responses and actively participate in class activities. 
                    <!-- If you're interested, please send an email to <a href="mailto:dl_ai599@navercorp.com">dl_ai599@navercorp.com </a> . -->
                  </strong></p></li>
                  <li><p style="color:red"><strong>
                    3/3: We are accepting extra enrollments, but spaces are limited to total of 48 students. 
                    <!-- If you're interested in taking this class, please send an email to <a href="mailto:dl_ai599@navercorp.com">dl_ai599@navercorp.com </a>. Current headcount: 40/48 -->
                  </strong></p></li>
	              	<li>2/28: Welcome to the deep learning and real-world applications class! We're still finalizing the schedule and the reading list. Stay tuned! </li>
	              </ul>
	            </div>
	          </div>         
	        </div>        
        </div>
    </div>


  <div class="row">
    <div class="col-md-12">
          <div class="bs-component">
            <div class="panel panel-primary">
              <div class="panel-heading">
                <h3 class="panel-title">Schedule</h3>
              </div>
              <div class="panel-body">
                <table class="table table-striped">
                  <tr>
                    <th>Week</th>
                    <th>Date</th>
                    <th class="topic">Topic</th>
                    <th class="topic">Invited Speaker</th>
                    <th class="reading">Reading <em><small>(<span class="label label-primary">response</span> indicates a reading response is required for one of the two articles.)</em></small></th>
                    <th>Due</th>
                  </tr>
                  <tr>
                    <td>1</td>
                    <td>3/4</td>
                    <td>Introduction & Course Overview</td>
                    <td>Ïú§ÏÉÅÎëê</td>
                    <td> <strong> please read the updated course syllabus, and please ask any questions you might have. </strong></td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>3/11</td>
                    <td>Representation learning in computer vision <br/> 
                      Session 1: Backbone architectures for computer vision
                      <br/></td>
                    <td>ÌóàÎ≥ëÌò∏</td>
                    <td> 
                      (1) <span class="label label-primary">response 1</span> Kornblith, Simon, et al. <a href="https://arxiv.org/abs/1805.08974">"Do better imagenet models transfer better?"</a>, CVPR 2019 <br/>
                      (2) <span class="label label-primary">response 1</span> Dosovitskiy, Alexey, et al. <a href="https://arxiv.org/abs/2010.11929">"An image is worth 16x16 words: Transformers for image recognition at scale."</a> ICLR 2021 <br/>
                      <br/>
                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Han, Dongyoon, et al. <a href="https://arxiv.org/abs/2007.00992">"Rethinking channel dimensions for efficient model design." </a>, CVPR 2021 </li>
                        <li> Heo, Byeongho, et al. <a href="https://arxiv.org/abs/2103.16302">"Rethinking spatial dimensions of vision transformers."</a>, ICCV 2021 </li>
                      </ul>                    
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>3/11</td>
                    <td>Representation learning in computer vision <br/>
                      Session 2: Training strong and robust vision models<br/></td>
                    <td>Ïú§ÏÉÅÎëê</td>
                    <!-- "(reading response only) Zhang, Hongyi, et al.,""mixup: Beyond Empirical Risk Minimization"", ICLR 2018 https://openreview.net/pdf?id=r1Ddp1-Rb
                          (reading response only) Shankar, Vaishaal, et al, ""Evaluating Machine Accuracy on ImageNet"", ICML 2020 http://proceedings.mlr.press/v119/tsipras20a/tsipras20a.pdf" -->
                    <td> 
                      (1) <span class="label label-primary">response 2</span> Zhang, Hongyi, et al. <a href="https://openreview.net/pdf?id=r1Ddp1-Rb"> "mixup: Beyond Empirical Risk Minimization." </a>, ICLR 2018<br/>
                      (2) <span class="label label-primary">response 2</span> Shankar, Vaishaal, et al. <a href="http://proceedings.mlr.press/v119/shankar20c/shankar20c.pdf"> "Evaluating Machine Accuracy on ImageNet." </a>, ICML 2020 <br/><br/>
                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li>He, Tong, et al. <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks_CVPR_2019_paper.pdf">"Bag of tricks for image classification with convolutional neural networks." </a>, CVPR 2019.</li>
                        <li>Wightnam, Ross et al., <a href="https://arxiv.org/abs/2110.00476">"ResNet strikes back: An improved training procedure in timm." </a>, arXiv 2021.</li>
                        <li>Yun, Sangdoo, et al. <a href="https://arxiv.org/abs/1905.04899">"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features." </a>, ICCV 2019</li>
                        <li>Yun, Sangdoo, et al. <a href="https://arxiv.org/abs/2101.05022"> "Re-labeling imagenet: from single to multi-labels, from global to localized labels." </a>, CVPR 2021</li>
                      </ul>                    
                    </td>
                    <td></td>
                  </tr>                
                  <tr>
                    <tr>
                      <td>3</td>
                      <td>3/18</td>
                      <td>Multimodal representation learning<br/>
                      Session 1: Multimodal deep learning</td>
                      <td>ÍπÄÏßÑÌôî</td>
                      <td> 
                        (1) <span class="label label-primary">response 1</span> Kim, Jin-Hwa, Jaehyun Jun, and Byoung-Tak Zhang. <a href=" http://arxiv.org/abs/1805.07932"> "Bilinear attention networks."</a>, NeurIPS 2018
                        <br/> 
                        (2) <span class="label label-primary">response 1</span> Anderson, Peter, et al. <a href="http://arxiv.org/abs/1707.07998"> "Bottom-up and top-down attention for image captioning and visual question answering."</a>, CVPR 2018<br/><br/>  
  
                        <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li>Ngiam, Jiquan, et al.<a href=" https://people.csail.mit.edu/khosla/papers/icml2011_ngiam.pdf"> "Multimodal deep learning."</a>, ICML 2011</li>
                          <li>Goyal et al. <a href="https://arxiv.org/abs/1612.00837"> "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"</a>, CVPR 2017  </li>
                          <li>Hudson, Drew A., and Christopher D. Manning. <a href=" http://arxiv.org/abs/1902.09506"> "GQA: a new dataset for compositional question answering over real-world images"</a>. CVPR 2019</li>
                        </ul>
                      </td>
                      <td></td>
                    </tr>
                    <tr>
                      <td>3</td>
                      <td>3/18</td>
                      <td>Multimodal representation learning <br/>
                      Session 2: Vision-and-Language Pre-training</td>
                      <td>ÍπÄÏõêÏû¨</td>
                      <td>
                        (1) <span class="label label-primary">response 2</span> Lu, Jiasen, et al. "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks." NeurIPS 2019 <br/>
                        (2) <span class="label label-primary">response 2</span> Kim, Wonjae, Bokyung Son, and Ildoo Kim. "Vilt: Vision-and-language transformer without convolution or region supervision." ICML 2021 <br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li>Chen, Yen-Chun, et al. "Uniter: Universal image-text representation learning." ECCV 2020</li>
                          <li>Singh, Amanpreet, et al. "FLAVA: A Foundational Language And Vision Alignment Model." arXiv 2021</li>
                          <li>Li, Junnan, et al. "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation." arXiv 2022</li>
                        </ul>
                      </td>
                      <td></td>
                    </tr>

                    <tr>
                      <td>4</td>
                      <td>3/25</td>
                      <td>Generative models</td>
                      <td>ÍπÄÏú§ÏßÄ </td>
                      <td>
                        (1) <span class="label label-primary">response 1</span> Invariant Information Clustering for Unsupervised Image Classification and Segmentation, Ji et al., ICCV, 2019<br/>
                        (2) <span class="label label-primary">response 1</span> SCAN: Learning to Classify Images without Labels, Gansbeke et al., ECCV, 2020<br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, Chen et al., NeurIPS, 2016</li>
                          <li>Finegan: Unsupervised hierarchical disentanglement for fine-grained object generation and discovery, Singh et al., CVPR, 2019</li>
                          <li>Contrastive Fine-grained Class Clustering via Generative Adversarial Networks, Kim et al., ICLR, 2022</li>
                        </ul>
                      </td>
                      <td></td>
                    </tr>  
                    <tr>
                      <td>4</td>
                      <td>3/25</td>
                      <td>Generative models</td>
                      <td> ÍπÄÏ§ÄÌò∏ </td>
                      <td>
                        (1) <span class="label label-primary">response 2</span> ContraGAN: Contrastive Learning for Conditional Image Generation (NeurIPS 2020)<br/>
                        (2) <span class="label label-primary">response 2</span> Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (ICLR 2021) <br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li>Improved Transformer for High-Resolution GANs (ICLR 2021)
                            </li>
                          <li>Analyzing and Improving the Image Quality of StyleGAN (CVPR 2020)
                            </li>
                          <li>Consistency Regularization for Generative Adversarial Networks (ICLR 2020)
                           </li>
                           <li>Feature Statistics Mixing Regularization for Generative Adversarial Networks (arXiv)</li>
                        </ul>
                      </td>
                      <td></td>
                    </tr>    

                    <td>5</td>
                    <td>4/1</td>
                    <td>Towards reliable machine learning<br/>
                    Session 1: Definition and real examples of shortcut learning</td>
                    <td>Ï†ÑÏÉÅÌòÅ</td>
                    <td>(1) <span class="label label-primary">response 1</span> Brendel, et al. <a href="https://arxiv.org/abs/1904.00760"> "Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet"</a>, ICLR 2019   <br/>
                      (2) <span class="label label-primary">response 1</span> Geirhos, et al. <a href=" https://arxiv.org/abs/1811.12231"> "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness"</a>, ICLR 2019
                      <br/><br/>


                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Geirhos, et al. <a href="https://arxiv.org/abs/2004.07780"> Shortcut Learning in Deep Neural Networks‚Äù </a>, Nature Machine Intelligence 2020. </li> 
                        <li> Scimeca, et al.<a href="https://arxiv.org/abs/2110.03095"> ‚ÄúWhich Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective‚Äù</a>, ICLR 2022.</li> 
                        <li>de Vries, Terrance, et al.<a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/de_Vries_Does_Object_Recognition_Work_for_Everyone_CVPRW_2019_paper.pdf"> "Does object recognition work for everyone?."</a> CVPR Workshops. 2019.</li> 
                    </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>5</td>
                    <td>4/1</td>
                    <td>Towards reliable machine learning<br/>
                    Session 2: Attempts to mitigate shortcut learning</td>
                    <td>Ï†ÑÏÉÅÌòÅ</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Madry, et al.<a href=" https://arxiv.org/abs/1706.06083"> "Towards Deep Learning Models Resistant to Adversarial Attacks"</a>, ICLR 2018<br/>
                      (2) <span class="label label-primary">response 2</span> Ganin, et al. <a href=" https://jmlr.org/papers/volume17/15-239/15-239.pdf"> "Domain-Adversarial Training of Neural Networks"</a>, JMLR 2016
                      <br/><br/>


                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li>Bahng, et al. <a href="https://arxiv.org/abs/1910.02806"> ‚ÄúLearning De-biased Representations with Biased Representations‚Äù</a>, ICML 2020. 
                          </li>
                        <li>Nam, et al. <a href="https://arxiv.org/abs/2007.02561"> "Learning from Failure: Training Debiased Classifier from Biased Classifier"</a>, NeurIPS 2020. 
                          </li>
                        <li>Cha, et al. <a href="https://arxiv.org/abs/2102.08604"> "SWAD: Domain Generalization by Seeking Flat Minima"</a>, NeurIPS 2021. </li>

                    </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>6</td>
                    <td>4/8</td>
                    <td>Practical scenarios and applications in computer vision <br/>
                      Session 1: TBD<br/></td>
                    <td>Ïú†ÏòÅÏ§Ä</td>
                    <td> (1) <span class="label label-primary">response 2</span> An, Xiang, et al. "Partial fc: Training 10 million identities on a single machine.", ICCV 2021<br/>
                      (2) <span class="label label-primary">response 2</span> Sculley, David, et al. "Hidden technical debt in machine learning systems.", NeurIPS 2015 <br/><br/>
                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Guo, Yandong, et al. "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition.", ECCV 2016</li>
                        <li> Zhu, Zheng, et al. "Webface260m: A benchmark unveiling the power of million-scale deep face recognition.", CVPR 2021</li>
                      </ul>                    
                    </td>
                    <td></td>
                  </tr>                
                  <tr>
                  <tr>
                    <td>6</td>
                    <td>4/8</td>
                    <td>Practical scenarios and applications in computer vision <br/>
                      Session 2: TBD<br/><br/></td>
                    <td> ÏúÑÎèôÏú§</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> SlowFast network for action recognition, 2019, ICCV
                    <br/>
                    (2) <span class="label label-primary">response 2</span>Non-local neural network, 2018, CVPR  <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset, 2017, CVPR </li>
                      <li> AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions, Arxiv </li>
                      <li> Regularization on Spatio-Temporally Smoothed Feature for Action Recognition, 2020, CVPR</li>
                    </ul>
                  </td>
                    <td></td>
                  </tr>                  
                  <tr>
                    <td>7</td>
                    <td>4/15</td>
                    <td>Practical scenarios and applications in computer vision <br/>
                      Session 3: TBD<br/><br/></td>
                    <td>Î∞±ÏòÅÎØº</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span>Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer, Arxiv
                   <br/>
                    (2) <span class="label label-primary">response 1</span>   Character region awareness for text detection, CVPR2019<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> What is wrong with scene text recognition model comparisons? dataset and model analysis, ICCV 2019
                       </li>
                      <li> Character Region Attention For Text Spotting, ECCV2020</li>
                      <li> TBD </li>
                    </ul>
                  </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>7</td>
                    <td>4/15</td>
                    <td>Practical scenarios and applications in computer vision <br/>
                      Session 4: TBD<br/><br/></td>
                    <td>Ïù¥Î∞îÎèÑ</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> Cha et al., Few-shot Compositional Font Generation with Dual Memory, ECCV 2020, https://arxiv.org/abs/2005.10510
                    <br/>
                    (2) <span class="label label-primary">response 2</span>Park et al., Few-shot Font Generation with Localized Style Representations and Factorization, AAAI 2021, https://arxiv.org/abs/2009.11042 <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li>Park et al., Multiple Heads are Better than One:Few-shot Font Generation with Multiple Localized Experts, ICCV 2021, https://arxiv.org/abs/2104.00887 </li>
                      <li> TBD </li>
                    </ul>
                  </td>
                    <td></td>
                  </tr>

                  
                  
                  <tr>
                    <td>8</td>
                    <td>4/22</td>
                    <td class="no-class">No Class (Midterm exams)</td>
                    <td class="no-class"></td>
                    <td></td>
                    <td></td>
                  </tr>

                  <tr>
                    <td>9</td>
                    <td>4/29</td>
                    <td>Speech recognition and applications                  </td>
                    <td>ÍπÄÌïúÍ∑ú</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> HuBERT: Self-supervised representation learning for speech recognition, generation, and compression        https://arxiv.org/pdf/2106.07447.pdf
                     <br/>
                      (2) <span class="label label-primary">response 1</span> W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training        https://arxiv.org/abs/2108.06209 <br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations        https://arxiv.org/pdf/2006.11477.pdf
                          </li>
                        <li> <a href="https://ieeexplore.ieee.org/abstract/document/9414641?casa_token=uUk0kPjEI74AAAAA:fO38_t_a3xOyxY2sZ_Lvt35vqWGTr-IgyouzipNeOd9xZV9FlhCdgZvA7D7a2_2LsC8BdEbm_AMecg" >Self-training and Pre-training are Complementary for Speech Recognition </a>
                          </li>
                        <li>Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition        https://arxiv.org/abs/2010.10504‚Äã</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>9</td>
                    <td>4/29</td>
                    <td>Speech recognition and applications                  </td>
                    <td> Ï†ïÎÇ®Í∑ú</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span>Conformer: Convolution-augmented Transformer for Speech Recognition https://arxiv.org/abs/2005.08100
                       <br/>
                      (2) <span class="label label-primary">response 2</span> ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context https://arxiv.org/abs/2005.03191<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li>Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks link
                          </li>
                        <li>Deep Speech 2: End-to-End Speech Recognition in English and Mandarin http://proceedings.mlr.press/v48/amodei16.html
                          </li>
                        <li>Sequence transduction with recurrent neural networks https://arxiv.org/pdf/1211.3711.pdf</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>

                  <tr>
                    <td>10</td>
                    <td>5/6</td>
                    <td>Voice synthesis and applications  <br/>
                    <td>ÏÜ°ÏùÄÏö∞</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span>  Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions
                      <br/>
                      (2) <span class="label label-primary">response 1</span>  FastSpeech 2 - Fast and High-quality End-to-end Text-to-Speech<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li>Tacotron - Towards End-to-end Speech Synthesis</li>
                        <li>Neural Speech Synthesis with Transformer Network</li>
                        <li>FastSpeech - Fast, Robust and Controllable Text to Speech</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <td>10</td>
                  <td>5/6</td>
                  <td>Voice synthesis and applications  <br/>
                  <td>Ìô©ÎØºÏ†ú</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span>  MelGAN - Generative Adversarial Networks for Conditional Waveform Synthesis<br/>
                    (2) <span class="label label-primary">response 2</span> Parallel WaveGAN - A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> WaveNet - A Generative Model for Raw Audio</li>
                      <li> Parallel WaveNet- Fast High-Fidelity Speech Synthesis</li>
                      <li> HiFi-GAN- Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>

                <tr>
                  <td>11</td>
                  <td>5/13</td>
                  <td>Large-scale user modeling and its applications</td>
                  <td>Í≥ΩÌïòÎÖπ</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span> Shin et al. "Scaling Law for Recommendation Models: Towards General-purpose User Representations", arXiv 2021. https://arxiv.org/abs/2111.11294
                   <br/>
                    (2) <span class="label label-primary">response 1</span> Shin et al. "One4all user representation for recommender systems in e-commerce", arXiv 2021. https://arxiv.org/abs/2106.00573 <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li>TBD</li>
                    </ul>
                  </td>
                  <td></td>
                </tr> 
                <tr>
                  <td>11</td>
                  <td>5/13</td>
                  <td>Large-scale user modeling and its applications</td>
                  <td> Ï†ïÏßÄÏàò</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span>Hsieh et al. "Collaborative Metric Learning", WWW 2017. https://dl.acm.org/doi/10.1145/3038912.3052639
                     <br/>
                    (2) <span class="label label-primary">response 2</span>"Section 5: Discussion on Industrial Impacts" in "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"  <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li>Tran et al., "Hierarchical Latent Relation Modeling for Collaborative Metric Learning", RecSys 2021. https://dl.acm.org/doi/fullHtml/10.1145/3460231.3474230
                       </li>
                      <li> OpenAI, "GPT-3 Powers the Next Generation of Apps", OpenAI Blog 2021. https://openai.com/blog/gpt-3-apps/
                        </li>
                      <li>Verduzco, "Best GPT-3 Tools, Examples and Use Cases", 2021. https://nogood.io/2021/06/25/gpt-3-tools/</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>                 
                  
                <tr>
                  <td>12</td>
                  <td>5/20</td>
                  <td>AutoML and Practical MLOps  <br/>
                  <td>ÍπÄÏßÄÌõà</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span>AutoML-Zero: Evolving Machine Learning Algorithms From Scratch: https://arxiv.org/pdf/2003.03384.pdf<br/>
                    (2) <span class="label label-primary">response 1</span> BOHB: Robust and Efficient Hyperparameter Optimization at Scale: https://arxiv.org/abs/1807.01774.pdf<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      Neural Architecture Search:
                      <li>DARTS: https://arxiv.org/pdf/1806.09055.pdf</li>
                      <li> NAS-Bench-201: https://arxiv.org/pdf/2001.00326.pdf</li>
                        Hyperparameter Optimization:
                      <li>* Hyperband: https://arxiv.org/pdf/1603.06560.pdf</li>
                        <li>* Population based training: https://arxiv.org/pdf/1711.09846.pdf</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>
                <tr>
                  <td>12</td>
                  <td>5/20</td>
                  <td>AutoML and Practical MLOps  <br/>
                  <td>ÏÑúÎèôÌïÑ</td>
                  <td>
                    No reading for his session
                  </td>
                  <td></td>
                </tr>
                  
              <tr>
                  <td>13</td>
                  <td>5/27</td>
                  <td>NLP, Dialogues, and QA <br/>                    </td>
                  <td>Ïù¥ÏÉÅÏö∞</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span> Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", NAACL 2019. https://arxiv.org/abs/1810.04805
                    <br/>
                    (2) <span class="label label-primary">response 1</span> Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", JMLR 2020. https://arxiv.org/abs/1910.10683<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li>Radford et al., "Language Models are Unsupervised Multitask Learners", 2019.
                        </li>
                      <li>GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation, https://arxiv.org/abs/2104.08826, ACL Findings, 2021.
                        </li>
                      <li>NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation, https://arxiv.org/abs/2105.14454, ACL, 2021.</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>
                <tr>
                  <td>13</td>
                  <td>5/27</td>
                  <td>NLP, Dialogues, and QA <br/>                    </td>
                  <td>ÍπÄÏÑ±Îèô</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> Recipes for building an open-domain chatbot (Roller et al., 2020) https://arxiv.org/abs/2004.13637
                    <br/>
                    (2) <span class="label label-primary">response 2</span>  Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020) https://arxiv.org/abs/2005.11401<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering (Izacard and Grave, 2020)  https://arxiv.org/abs/2007.01282</li>
                      <li>Retrieval Augmentation Reduces Hallucination in Conversation (Shuster et al., 2021) https://arxiv.org/abs/2104.07567</li>
                      <li>Beyond Goldfish Memory‚àó : Long-Term Open-Domain Conversation (Xu et al., 2021) https://arxiv.org/abs/2107.07567</li>
                        <li>Improving language models by retrieving from trillions of tokens (Borgeaud et al., 2021) https://arxiv.org/abs/2112.04426</li>
                        <li>Saving Dense Retriever from Shortcut Dependency in Conversational Search (Kim et al., 2022) https://arxiv.org/abs/2202.07280</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>                  
                  
              <tr>
                  <td>14</td>
                  <td>6/3</td>
                  <td>Hyperscale LM & NLP applications</td>
                  <td>Ïù¥Í∏∞Ï∞Ω</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span> Brown et al., "Language Models are Few-Shot Learners", NeurIPS 2021, https://arxiv.org/abs/2005.14165
                    <br/>
                    (2) <span class="label label-primary">response 1</span>  Rae et al., "Scaling Language Models: Methods, Analysis & Insights from Training Gopher", arxiv 2021. https://arxiv.org/abs/2112.11446 <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li>Smith et al., "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model", arXiv 2022. https://arxiv.org/abs/2201.11990
                        </li>
                      <li>Tay et al., "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers", ICLR 2022. https://arxiv.org/abs/2109.10686
                        </li>
                      <li>Kim et al., "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers", EMNLP 2021.</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>
                <tr>
                  <td>14</td>
                  <td>6/3</td>
                  <td>Hyperscale LM & NLP applications</td>
                  <td>Ïú†Í∞ïÎØº</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> The Power of Scale for Parameter-Efficient Prompt Tuning (https://arxiv.org/abs/2104.08691)
                    <br/>
                    (2) <span class="label label-primary">response 2</span>Prefix-Tuning: Optimizing Continuous Prompts for Generation (https://arxiv.org/abs/2101.00190) <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li>Towards a Unified View of Parameter-Efficient Transfer Learning (https://arxiv.org/abs/2110.04366)
                        </li>
                      <li> LoRA: Low-Rank Adaptation of Large Language Models (https://arxiv.org/abs/2106.09685)
                        </li>
                      <li>It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners (https://arxiv.org/abs/2009.07118)</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>                  
                  
                <tr>
                  <td>15</td>
                  <td>6/10</td>
                  <td>Human-centric NLP</td>
                  <td>Ïù¥ÌôîÎûÄ</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span>Dinan, Emily, et al. "Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation." Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.
                      <br/>
                    (2) <span class="label label-primary">response 1</span> Perez, Ethan, et al. "Red Teaming Language Models with Language Models." arXiv preprint arXiv:2202.03286 (2022).<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li>Bender, Emily M., et al. "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ü¶ú." Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021.
                        </li>
                      <li>Liu, Haochen, et al. "Does Gender Matter? Towards Fairness in Dialogue Systems." Proceedings of the 28th International Conference on Computational Linguistics. 2020.
                        </li>
                      <li>Liu, Haochen, et al. "Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning." Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.
                        </li>
                        <li>Sheng, Emily, et al. "‚ÄúNice Try, Kiddo‚Äù: Investigating Ad Hominems in Dialogue Responses." Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.
                          </li>
                          <li>Ma, Xinyao, et al. "PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction." Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.
                            </li>
                            <li>Xu, Albert, et al. "Detoxifying Language Models Risks Marginalizing Minority Voices." Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>

                  <tr>
                    <td>15</td>
                    <td>6/10</td>
                    <td>Human-centric NLP</td>
                    <td>Ï†ïÏ§ÄÏòÅ, Ïù¥ÎØºÏïÑ</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> TBD <br/>
                      (2) <span class="label label-primary">response 2</span> TBD <br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li></li>
                        <li></li>
                        <li></li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>                  
                  
                                    
                  <tr>
                    <td>16</td>
                    <td>6/17</td>
                    <td class='no-class'>No Class (Final exams)</td>
                    <td class="no-class"></td>
                    <td></td>
                    <td></td>
                  </tr> 
                                                                                                                                              
                </table>
              </div>
            </div>         
          </div>        
        </div>
    </div>                  

	<div class="row">
    <div class="col-md-4">
          <div class="bs-component">
            <div class="panel panel-primary">
              <div class="panel-heading">
                <h3 class="panel-title">Topics (tentative)</h3>
              </div>
              <div class="panel-body">
                Major topics include:
                <ul>
                 <li>Representation Learning</li>
                 <li>Reliable ML</li>
                 <li>Voice and Speech</li>
                 <li>NLP</li>
                 <li>MLOps</li>
                 <li>Recommendation systems</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Grading</h3>
	            </div>
	            <div class="panel-body">
	            <ul>
                <li>Attendance: 20%</li>
      					<li>Reading responses: 40%</li>
      					<li>Topic presentation: 20%</li>
      					<li>Class participation: 10%</li>
                <li>Quizes: 10%</li>
				      </ul>

      				<strong>Late policy</strong>: Three lowest reading response grades will be removed. No late submissions are allowed for the reading responses.
	            </div>
	          </div>         
	        </div>	        
        </div>
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Prerequisites</h3>
	            </div>
	            <div class="panel-body">
	            There are no official course prerequisites. But assignments involve a lot of reading, research experience in machine learning is useful, but not required. 
	            </div>
	          </div>         
	        </div>	        
        </div>   
    </div>

</div>

<script src="//code.jquery.com/jquery-1.10.2.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="dist/js/ripples.min.js"></script>
<script src="dist/js/material.min.js"></script>
<script>
  $(function () {
    $.material.init();
  });
</script>

</body>
</html>
