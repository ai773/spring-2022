<html>
<head>
  <meta charset="utf-8"/>
  <!-- Material Design fonts -->
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto:300,400,500,700">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/icon?family=Material+Icons">

  <!-- Bootstrap -->
  <!-- <link rel="stylesheet" type="text/css" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css"> -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  <!-- Bootstrap Material Design -->
  <link rel="stylesheet" type="text/css" href="dist/css/bootstrap-material-design.css">
  <link rel="stylesheet" type="text/css" href="dist/css/ripples.min.css">
  <style>
    body {
    	padding-top: 70px;
    	font-weight: normal;
    }
    th {
    	font-weight: bold;
    }
	th, td {
	    font-size: 14px;
	    padding: 10px;
	    border: solid 1px lightgrey;
	}
	.topic {

	}
	.reading {
		max-width: 450px;
	}
	.no-class {
		background-color: lightgrey;
	}
  </style>
  <title>AI733: Special Topics in Artificial Intelligence - Deep Learning and Real-world Applications | Spring 2022</title>
</head>
<body>
	

<div class="navbar navbar-default navbar-fixed-top">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">AI733</a>
    </div>
    <div class="navbar-collapse collapse navbar-responsive-collapse">
      <ul class="nav navbar-nav">
        <li class="active"><a href="index.html">Home</a></li>
        <li><a href="reading-response.html">Reading Response</a></li>
        <li><a href="topic-presentation.html">Topic Presentation</a></li>
        <li><a href="class-logistics.html">Class Logistics</a></li>
      </ul>
    </div>
  </div>
</div>


<div class="container-fluid main">
	<div class="row">
		<div class="col-md-12">
          <div class="well">
            <div>SNU CSE Spring 2022</div>
            <h1> AI773: Special Topics in Artificial Intelligence - Deep Learning and Real-world Applications  (4190.773.001)</h1>

            <p> Deep learning is now an integral part of daily systems and tools people use, and therefore no longer is a concern of only academic research. You will get the front-row experience on practical issues in research and development of deep learning systems from the leading experts and researchers. Major course activities include:</p>

            <ul>
              <li><strong>Reading Response</strong>: You'll read and discuss important papers and articles in the field. Each week, there will be 1-2 reading assignments, for which you'll write a short response to.</li>
              <li><strong>Topic Presentation</strong>: Once a semester, you'll lead the class by summarizing the readings, and spurring the in-class discussion. </li>
              <li><strong>In-class Activities</strong>: Each class will feature activities that will help you understand core concepts introduced in the course. </li>
            </ul>

          </div>
        <div id="source-button" class="btn btn-primary btn-xs" style="display: none;">&lt; &gt;</div></div>		
	</div>

	<div class="row">
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Course Staff</h3>
	            </div>
	            <div class="panel-body">
	              <strong>Instructor: </strong><br/>
                  &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://sangdooyun.github.io/">Prof. Sangdoo Yun</a><br/>
               <br/>
	              <!-- &nbsp;&nbsp;&nbsp;&nbsp;<em>Office Hours</em>: 4-5pm Tue/Thu @ N1-605<br/><br/> -->
	              <strong>TAs: </strong></br>
                  &nbsp;&nbsp;&nbsp;&nbsp; <a href="">Ïú†ÏäπÎ£° </a><br/>
                  &nbsp;&nbsp;&nbsp;&nbsp; <a href="">Í∞ïÎ¥âÍ∑† </a><br/><br/>
                <strong>Staff Mailing List</strong>:<br/>&nbsp;&nbsp;&nbsp;&nbsp;  <a href="mailto:dl_ai773@navercorp.com">dl_ai773@navercorp.com</a> <br/>
                &nbsp;&nbsp;&nbsp;&nbsp; note: this is a group email address that includes the instructors and the TAs. 
                 <!-- <a href="mailto:cs492@kixlab.org">cs492@kixlab.org</a> -->
	            </div>
	          </div>         
	        </div>	     
        </div>         
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Time &amp; Location</h3>
	            </div>
	            <div class="panel-body">
	              <strong>When</strong>: 10:00am-12:45pm Fri<br/>
	              <strong>Where</strong>: Zoom (through <a href="http://etl.snu.ac.kr/course/view.php?id=213145">ETL</a>)
	            </div>
	          </div>         
	        </div>        
        </div> 
    
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Links</h3>
	            </div>
	            <div class="panel-body">
	              <strong>Course Website</strong>: <a href="https://ai773.github.io/spring-2022/">https://ai773.github.io/spring-2022/</a><br/>
	              <strong>Submission &amp; Grading</strong>: <a href="http://etl.snu.ac.kr/course/view.php?id=213145">ETL</a><br/>
	              <strong>QnA</strong>: ETL or email <a href=""></a>
	            </div>
	          </div>         
	        </div>        
        </div>
    </div>

	<div class="row">
		<div class="col-md-12">
	        <div class="bs-component">
	          <div class="panel panel-info">
	            <div class="panel-heading">
	              <h3 class="panel-title">Updates</h3>
	            </div>
	            <div class="panel-body">
	              <ul>
                  <li>
                    <!-- <p style="color:red"> -->
                    <strong>
                    [TODO] 3/10: Upload excellent reading response examples.
                  </strong>
                <!-- </p> -->
                  </li>
                  <li>
                    <p style="color:red">
                    <strong>
                      3/4: Choose the papers you want to present, please fill in 
                      <a href="https://docs.google.com/forms/d/e/1FAIpQLSc2O-ndhdWZ_maA4OtuWIFz5NnWO_MPdDfVI_FYk7sWey5-UQ/viewform">
                        this survey
                      </a> (due: 3/10). Current status (3/6): 28 done among 62 participants.
                    </strong>             
                  </p>       
                  </li>
                  <li> 
                    <!-- <strong>  -->
                     3/4: Extra-enrollments and auditing applications are closed.
                  <!-- </strong>                 -->
                  </li>
                  <!-- <li>
                    3/3 We are accepting extra enrollments, but spaces are limited to total of 48 students. (due: 3/3 11:59 PM)
                  </li>
                  <li>
                    3/3: You may "audit" or "sit in" this class, but you still have to submit reading responses and actively participate in class activities. (due: 3/3 11:59 PM)
                  </li> -->
	              	<li>2/28: Welcome to the deep learning and real-world applications class! We're still finalizing the schedule and the reading list. Stay tuned! </li>
	              </ul>
	            </div>
	          </div>         
	        </div>        
        </div>
    </div>


  <div class="row">
    <div class="col-md-12">
          <div class="bs-component">
            <div class="panel panel-primary">
              <div class="panel-heading">
                <h3 class="panel-title">Schedule</h3>
              </div>
              <div class="panel-body">
                <table class="table table-striped">
                  <tr>
                    <th>Week</th>
                    <th>Date</th>
                    <th class="topic">Topic</th>
                    <th class="topic">Invited Speaker</th>
                    <th class="reading">Reading <em><small>(<span class="label label-primary">response</span> indicates a reading response is required for one of the two articles.)</em></small></th>
                    <th>Due</th>
                  </tr>
                  <tr>
                    <td>1</td>
                    <td>3/4</td>
                    <td>Introduction & Course Overview 
                      <a href="https://docs.google.com/presentation/d/1g1uxDBD4nwKM3VNtjGIjkkg7D7oa0oU5rM_AO-_ZPmc/edit#slide=id.p">[slide]</a>
                    </td>
                    <td>Ïú§ÏÉÅÎëê</td>
                    <td> <strong> please read the updated course syllabus, and please ask any questions you might have. </strong></td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>3/11</td>
                    <td>Representation learning in computer vision <br/> 
                      Session 1: Backbone architectures for computer vision
                      <br/></td>
                    <td>ÌóàÎ≥ëÌò∏</td>
                    <td> 
                      (1) <span class="label label-primary">response 1</span> Kornblith, Simon, et al. <a href="https://arxiv.org/abs/1805.08974">"Do better imagenet models transfer better?"</a>, CVPR 2019 <br/>
                      (2) <span class="label label-primary">response 1</span> Dosovitskiy, Alexey, et al. <a href="https://arxiv.org/abs/2010.11929">"An image is worth 16x16 words: Transformers for image recognition at scale."</a> ICLR 2021 <br/>
                      <br/>
                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Han, Dongyoon, et al. <a href="https://arxiv.org/abs/2007.00992">"Rethinking channel dimensions for efficient model design." </a>, CVPR 2021 </li>
                        <li> Heo, Byeongho, et al. <a href="https://arxiv.org/abs/2103.16302">"Rethinking spatial dimensions of vision transformers."</a>, ICCV 2021 </li>
                      </ul>                    
                    </td>
                    <td>3/9</td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>3/11</td>
                    <td>Representation learning in computer vision <br/>
                      Session 2: Training strong and robust vision models<br/></td>
                    <td>Ïú§ÏÉÅÎëê</td>
                    <!-- "(reading response only) Zhang, Hongyi, et al.,""mixup: Beyond Empirical Risk Minimization"", ICLR 2018 https://openreview.net/pdf?id=r1Ddp1-Rb
                          (reading response only) Shankar, Vaishaal, et al, ""Evaluating Machine Accuracy on ImageNet"", ICML 2020 http://proceedings.mlr.press/v119/tsipras20a/tsipras20a.pdf" -->
                    <td> 
                      (1) <span class="label label-primary">response 2</span> Zhang, Hongyi, et al. <a href="https://openreview.net/pdf?id=r1Ddp1-Rb"> "mixup: Beyond Empirical Risk Minimization." </a>, ICLR 2018<br/>
                      (2) <span class="label label-primary">response 2</span> Shankar, Vaishaal, et al. <a href="http://proceedings.mlr.press/v119/shankar20c/shankar20c.pdf"> "Evaluating Machine Accuracy on ImageNet." </a>, ICML 2020 <br/><br/>
                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li>He, Tong, et al. <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks_CVPR_2019_paper.pdf">"Bag of tricks for image classification with convolutional neural networks." </a>, CVPR 2019.</li>
                        <li>Wightnam, Ross et al., <a href="https://arxiv.org/abs/2110.00476">"ResNet strikes back: An improved training procedure in timm." </a>, arXiv 2021.</li>
                        <li>Yun, Sangdoo, et al. <a href="https://arxiv.org/abs/1905.04899">"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features." </a>, ICCV 2019</li>
                        <li>Yun, Sangdoo, et al. <a href="https://arxiv.org/abs/2101.05022"> "Re-labeling imagenet: from single to multi-labels, from global to localized labels." </a>, CVPR 2021</li>
                      </ul>                    
                    </td>
                    <td>3/9</td>
                  </tr>                
                  <tr>
                    <tr>
                      <td>3</td>
                      <td>3/18</td>
                      <td>Multimodal representation learning<br/>
                      Session 1: Multimodal deep learning</td>
                      <td>ÍπÄÏßÑÌôî</td>
                      <td> 
                        (1) <span class="label label-primary">response 1</span> Kim, Jin-Hwa, Jaehyun Jun, and Byoung-Tak Zhang. <a href=" http://arxiv.org/abs/1805.07932"> "Bilinear attention networks."</a>, NeurIPS 2018
                        <br/> 
                        (2) <span class="label label-primary">response 1</span> Anderson, Peter, et al. <a href="http://arxiv.org/abs/1707.07998"> "Bottom-up and top-down attention for image captioning and visual question answering."</a>, CVPR 2018<br/><br/>  
  
                        <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li>Ngiam, Jiquan, et al.<a href=" https://people.csail.mit.edu/khosla/papers/icml2011_ngiam.pdf"> "Multimodal deep learning."</a>, ICML 2011</li>
                          <li>Goyal et al. <a href="https://arxiv.org/abs/1612.00837"> "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"</a>, CVPR 2017  </li>
                          <li>Hudson, Drew A., and Christopher D. Manning. <a href=" http://arxiv.org/abs/1902.09506"> "GQA: a new dataset for compositional question answering over real-world images"</a>. CVPR 2019</li>
                        </ul>
                      </td>
                      <td></td>
                    </tr>
                    <tr>
                      <td>3</td>
                      <td>3/18</td>
                      <td>Multimodal representation learning <br/>
                      Session 2: Vision-and-Language Pre-training</td>
                      <td>ÍπÄÏõêÏû¨</td>
                      <td>
                        (1) <span class="label label-primary">response 2</span> Lu, Jiasen, et al. <a href="https://arxiv.org/abs/1908.02265"> "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks."</a>, NeurIPS 2019 <br/>
                        (2) <span class="label label-primary">response 2</span> Kim, Wonjae, et al. <a href="https://arxiv.org/abs/2102.03334"> "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision."</a>, ICML 2021 <br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Chen, Yen-Chun, et al. <a href="https://arxiv.org/abs/1909.11740"> "UNITER: UNiversal Image-TExt Representation Learning."</a>, ECCV 2020</li>
                        <li> Singh, Amanpreet, et al. <a href="https://arxiv.org/abs/2112.04482"> "FLAVA: A Foundational Language And Vision Alignment Model."</a>, arXiv 2021</li>
                        <li> Li, Junnan, et al. <a href="https://arxiv.org/abs/2201.12086"> "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation."</a>, arXiv 2022</li>
                      </ul>
                      </td>
                      <td></td>
                    </tr>

                    <tr>
                      <td>4</td>
                      <td>3/25</td>
                      <td>Generative models <br/> Session 1</td>
                      <td>ÍπÄÏú§ÏßÄ </td>
                      <td>
                        (1) <span class="label label-primary">response 1</span> Ji, Xu, et al. <a href="https://arxiv.org/abs/1807.06653"> "Invariant Information Clustering for Unsupervised Image Classification and Segmentation."</a>, ICCV 2019<br/>
                        (2) <span class="label label-primary">response 1</span> Van, Gansbeke, et al. <a href="https://arxiv.org/abs/2005.12320"> "SCAN: Learning to Classify Images without Labels."</a>, ECCV 2020<br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li> Chen, Xi, et al. <a href="https://arxiv.org/abs/1606.03657"> "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets."</a>, NeurIPS 2016</li>
                          <li> Krishna, Kumar, Singh, et al. <a href="https://arxiv.org/abs/1811.11155"> "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery."</a>, CVPR 2019</li>
                          <li> Kim, Yunji and Ha, Jung-Woo. <a href="https://arxiv.org/abs/2112.14971"> "Contrastive Fine-grained Class Clustering via Generative Adversarial Networks."</a>, ICLR 2022</li>
                        </ul>
                      </td>
                      <td></td>
                    </tr>  
                    <tr>
                      <td>4</td>
                      <td>3/25</td>
                      <td>Generative models <br/> Session 2</td>
                      <td> ÍπÄÏ§ÄÌò∏ </td>
                      <td>
                        (1) <span class="label label-primary">response 2</span> Kang, Minguk and Park, Jaesik. <a href="https://arxiv.org/abs/2006.12681"> "ContraGAN: Contrastive Learning for Conditional Image Generation."</a> NeurIPS 2020<br/>
                        (2) <span class="label label-primary">response 2</span> Liu, Bingchen, et al. <a href="https://arxiv.org/abs/2101.04775"> "Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis."</a> ICLR 2021<br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li> Zhao, Long, et al. <a href="https://arxiv.org/abs/2106.07631"> "Improved Transformer for High-Resolution GANs."</a>, ICLR 2021</li>
                          <li> Karras, Tero, et al. <a href="https://arxiv.org/abs/1912.04958"> "Analyzing and Improving the Image Quality of StyleGAN."</a>, CVPR 2020</li>
                          <li> Zhang, Han, et al. <a href="https://arxiv.org/abs/1910.12027"> "Consistency Regularization for Generative Adversarial Networks."</a>, ICLR 2020</li>
                          <li> Kim, Junho, et al. <a href="https://arxiv.org/abs/2112.04120"> "Feature Statistics Mixing Regularization for Generative Adversarial Networks."</a>, arXiv 2021</li>
                        </ul>
                      </td>
                      <td></td>
                    </tr>    

                    <td>5</td>
                    <td>4/1</td>
                    <td>Towards reliable machine learning<br/>
                    Session 1: Definition and real examples of shortcut learning</td>
                    <td>Ï†ÑÏÉÅÌòÅ</td>
                    <td>(1) <span class="label label-primary">response 1</span> Brendel, et al. <a href="https://arxiv.org/abs/1904.00760"> "Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet"</a>, ICLR 2019   <br/>
                      (2) <span class="label label-primary">response 1</span> Geirhos, et al. <a href=" https://arxiv.org/abs/1811.12231"> "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness"</a>, ICLR 2019
                      <br/><br/>


                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Geirhos, et al. <a href="https://arxiv.org/abs/2004.07780"> Shortcut Learning in Deep Neural Networks‚Äù </a>, Nature Machine Intelligence 2020. </li> 
                        <li> Scimeca, et al.<a href="https://arxiv.org/abs/2110.03095"> ‚ÄúWhich Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective‚Äù</a>, ICLR 2022.</li> 
                        <li>de Vries, Terrance, et al.<a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/de_Vries_Does_Object_Recognition_Work_for_Everyone_CVPRW_2019_paper.pdf"> "Does object recognition work for everyone?."</a> CVPR Workshops. 2019.</li> 
                    </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>5</td>
                    <td>4/1</td>
                    <td>Towards reliable machine learning<br/>
                    Session 2: Attempts to mitigate shortcut learning</td>
                    <td>Ï†ÑÏÉÅÌòÅ</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Madry, et al.<a href=" https://arxiv.org/abs/1706.06083"> "Towards Deep Learning Models Resistant to Adversarial Attacks"</a>, ICLR 2018<br/>
                      (2) <span class="label label-primary">response 2</span> Ganin, et al. <a href=" https://jmlr.org/papers/volume17/15-239/15-239.pdf"> "Domain-Adversarial Training of Neural Networks"</a>, JMLR 2016
                      <br/><br/>


                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li>Bahng, et al. <a href="https://arxiv.org/abs/1910.02806"> ‚ÄúLearning De-biased Representations with Biased Representations‚Äù</a>, ICML 2020. 
                          </li>
                        <li>Nam, et al. <a href="https://arxiv.org/abs/2007.02561"> "Learning from Failure: Training Debiased Classifier from Biased Classifier"</a>, NeurIPS 2020. 
                          </li>
                        <li>Cha, et al. <a href="https://arxiv.org/abs/2102.08604"> "SWAD: Domain Generalization by Seeking Flat Minima"</a>, NeurIPS 2021. </li>

                    </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>6</td>
                    <td>4/8</td>
                    <td>Practical scenarios and applications in computer vision <br/>
                      Session 1<br/></td>
                    <td>Ïú†ÏòÅÏ§Ä</td>
                    <td> (1) <span class="label label-primary">response 1</span> An, Xiang, et al. <a href="https://arxiv.org/abs/2010.05222"> "Partial FC: Training 10 Million Identities on a Single Machine."</a>, ICCV 2021<br/>
                      (2) <span class="label label-primary">response 1</span> Sculley, David, et al. <a href="https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf"> "Hidden technical debt in machine learning systems."</a>, NeurIPS 2015 <br/><br/>
                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Guo, Yandong, et al. <a href="https://arxiv.org/abs/1607.08221"> "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition."</a>, ECCV 2016</li>
                        <li> Zhu, Zheng, et al. <a href="https://arxiv.org/abs/2103.04098"> "WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition."</a>, CVPR 2021</li>
                      </ul>                    
                    </td>
                    <td></td>
                  </tr>                
                  <tr>
                  <tr>
                    <td>6</td>
                    <td>4/8</td>
                    <td>Practical scenarios and applications in computer vision <br/>
                      Session 2<br/><br/></td>
                      <td> ÏúÑÎèôÏú§</td>
                      <td>
                        (1) <span class="label label-primary">response 2</span> Feichtenhofer, Christoph, et al. <a href="https://arxiv.org/abs/1812.03982"> "SlowFast Networks for Video Recognition."</a>, ICCV 2019
                        <br/>
                        (2) <span class="label label-primary">response 2</span> Wang, Xiaolong, et al. <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf"> "Non-local Neural Networks."</a>, CVPR 2018<br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li> Carreira, Joao and Zisserman, Andrew. <a href="https://arxiv.org/abs/1705.07750"> "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset."</a>, CVPR 2017</li>
                          <li> Cu, Chunhui, et al. <a href="https://arxiv.org/abs/1705.08421"> "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions."</a>, CVPR 2018 </li>
                          <li> Kim, Jinhyung, et al. <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Regularization_on_Spatio-Temporally_Smoothed_Feature_for_Action_Recognition_CVPR_2020_paper.pdf"> "Regularization on Spatio-Temporally Smoothed Feature for Action Recognition."</a>, CVPR 2020</li>
                        </ul>
                      </td>
                    <td></td>
                  </tr>                  
                  <tr>
                    <td>7</td>
                    <td>4/15</td>
                    <td>Practical scenarios and applications in computer vision <br/>
                      Session 1<br/><br/></td>
                      <td>Î∞±ÏòÅÎØº</td>
                      <td>
                        (1) <span class="label label-primary">response 1</span> Kittenplon, Yair, et al. <a href="https://arxiv.org/abs/2202.05508"> "Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer."</a>, arXiv 2022
                       <br/>
                        (2) <span class="label label-primary">response 1</span> Baek, Youngmin, et al. <a href="https://arxiv.org/abs/1904.01941"> "Character region awareness for text detection."</a>, CVPR 2019<br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li> Baek, Youngmin, et al. <a href="https://arxiv.org/abs/1904.01906"> "What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis."</a>, ICCV 2019</li>
                          <li> Baek, Youngmin, et al. <a href="https://arxiv.org/abs/2007.09629"> "Character Region Attention For Text Spotting"</a>, ECCV 2020</li>
                          <li></li>
                        </ul>
                      </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>7</td>
                    <td>4/15</td>
                    <td>Practical scenarios and applications in computer vision <br/>
                      Session 2<br/><br/></td>
                      <td>Ïù¥Î∞îÎèÑ</td>
                      <td>
                        (1) <span class="label label-primary">response 2</span> Cha, Junbum, et al. <a href="https://arxiv.org/abs/2005.10510"> "Few-shot Compositional Font Generation with Dual Memory."</a>, ECCV 2020
                        <br/>
                        (2) <span class="label label-primary">response 2</span> Park, Song, et al. <a href="https://arxiv.org/abs/2009.11042"> "Few-shot Font Generation with Localized Style Representations and Factorization."</a>, AAAI 2021 <br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li> Park, Song, et al. <a href="https://arxiv.org/abs/2104.00887"> "Multiple Heads are Better than One:Few-shot Font Generation with Multiple Localized Experts."</a>, ICCV 2021</li>
                          <!-- <li></li>
                          <li></li> -->
                        </ul>
                      </td>
                    <td></td>
                  </tr>

                  
                  
                  <tr>
                    <td>8</td>
                    <td>4/22</td>
                    <td class="no-class">No invited talk - Student presentations</td>
                    <td class="no-class"></td>
                    <td>TBD</td>
                    <td></td>
                  </tr>

                  <tr>
                    <td>9</td>
                    <td>4/29</td>
                    <td>Speech recognition and applications   <br/> Session 1               </td>
                    <td>ÍπÄÌïúÍ∑ú</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Hsu, Wei-Ning, et al. <a href="https://arxiv.org/abs/2106.07447"> "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units."</a>, IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021<br/>
                      (2) <span class="label label-primary">response 1</span> Chung, Yu-An, et al. <a href="https://arxiv.org/abs/2108.06209"> "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training."</a>, arXiv 2021<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Baevski, Alexei, et al. <a href="https://arxiv.org/abs/2006.11477"> "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations."</a>, NeurIPS 2020</li>
                        <li> Zoph, Barret, et al. <a href="https://proceedings.neurips.cc/paper/2020/file/27e9661e033a73a6ad8cefcde965c54d-Paper.pdf"> "Self-training and Pre-training are Complementary for Speech Recognition."</a>, NeurIPS 2020</li>
                        <li> Zhang, Yu, et al. <a href="https://arxiv.org/abs/2010.10504"> "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition."</a>, NeurIPS 2020 Workshop‚Äã</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>9</td>
                    <td>4/29</td>
                    <td>Speech recognition and applications  <br/> Session 2                </td>
                    <td>Ï†ïÎÇ®Í∑ú</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Culati, Anmol, et al. <a href="https://arxiv.org/abs/2005.08100"> "Conformer: Convolution-augmented Transformer for Speech Recognition."</a>, Interspeech 2020<br/>
                      (2) <span class="label label-primary">response 2</span> Han, Wei, et al. <a href="https://arxiv.org/abs/2005.03191"> "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context."</a>, Interspeech 2020<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Graves, Alex, et al. <a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf"> "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks."</a>, ICML 2006</li>
                        <li> Amodei, Dario, et al. <a href="https://arxiv.org/abs/1512.02595"> "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin."</a>, ICML 2016</li>
                        <li> Graves, Alex. <a href="https://arxiv.org/abs/1211.3711"> "Sequence Transduction with Recurrent Neural Networks."</a>, ICML 2012 Workshop</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>

                  <tr>
                    <td>10</td>
                    <td>5/6</td>
                    <td>Voice synthesis and applications <br/> Session 1 <br/>
                      <td>ÏÜ°ÏùÄÏö∞</td>
                      <td>
                        (1) <span class="label label-primary">response 1</span> Shen, Jonathan, et al. <a href="https://arxiv.org/abs/1712.05884"> "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions."</a>, ICASSP 2018
                        <br/>
                        (2) <span class="label label-primary">response 1</span> Ren, Yi, et al. <a href="https://arxiv.org/abs/2006.04558"> "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech."</a>, ICLR 2021<br/><br/>
                      
                      <strong>Recommended reading</strong> <br/>
                        <ul>
                          <li> Wang, Yuxuan, et al. <a href="https://arxiv.org/abs/1703.10135"> "Tacotron: Towards End-to-End Speech Synthesis."</a>, Interspeech 2017</li>
                          <li> Li, Naihan, et al. <a href="https://arxiv.org/abs/1809.08895"> "Neural Speech Synthesis with Transformer Network."</a>, AAAI 2019</li>
                          <li> Yi, Ren, et al. <a href="https://arxiv.org/abs/1905.09263"> "FastSpeech: Fast, Robust and Controllable Text to Speech."</a>, NeurIPS 2019</li>
                        </ul>
                      </td>
                    <td></td>
                  </tr>
                  <td>10</td>
                  <td>5/6</td>
                  <td>Voice synthesis and applications <br/> Session 2 <br/>
                    <td>Ìô©ÎØºÏ†ú</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Kumar, Kundan, et al. <a href="https://arxiv.org/abs/1910.06711"> "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis."</a>, NeurIPS 2019<br/>
                      (2) <span class="label label-primary">response 2</span> Yamamoto, Ryuichi, et al. <a href="https://arxiv.org/abs/1910.11480"> "Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram."</a>, ICASSP 2020<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Oord, Aaron van den, et al. <a href="https://arxiv.org/abs/1609.03499"> "WaveNet: A Generative Model for Raw Audio."</a>, arXiv 2016</li>
                        <li> Oord, Aaron van den, et al. <a href="https://arxiv.org/abs/1711.10433"> "Parallel WaveNet: Fast High-Fidelity Speech Synthesis."</a>, ICML 2018</li>
                        <li> Kong, Jungil, et al. <a href="https://arxiv.org/abs/2010.05646"> "HiFi-GAN- Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis."</a>, NeurIPS 2020</li>
                      </ul>
                    </td>
                  <td></td>
                </tr>

                <tr>
                  <td>11</td>
                  <td>5/13</td>
                  <td>Large-scale user modeling and its applications <br/> Session 1</td>
                  <td>Í≥ΩÌïòÎÖπ</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span> Shin, et al. <a href="https://arxiv.org/abs/2111.11294"> "Scaling Law for Recommendation Models: Towards General-purpose User Representations"</a>, arXiv 2021<br/>
                    (2) <span class="label label-primary">response 1</span> Shin, et al. <a href="https://arxiv.org/abs/2106.00573"> "One4all user representation for recommender systems in e-commerce"</a>, arXiv 2021<br/><br/>
                  
                  <!-- <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li></li>
                      <li></li>
                      <li></li>
                    </ul> -->
                  </td>
                  <td></td>
                </tr> 
                <tr>
                  <td>11</td>
                  <td>5/13</td>
                  <td>Large-scale user modeling and its applications <br/> Session 2</td>
                  <td> Ï†ïÏßÄÏàò</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> Hsieh, et al. <a href="https://dl.acm.org/doi/10.1145/3038912.3052639"> "Collaborative Metric Learning."</a>, WWW 2017<br/>
                    (2) <span class="label label-primary">response 2</span> Kim, Boseop, et al. <a href="https://arxiv.org/abs/2109.04650"> "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers."</a>, EMNLP 2021<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Tran, et al. <a href="https://arxiv.org/abs/2108.04655"> "Hierarchical Latent Relation Modeling for Collaborative Metric Learning."</a>, RecSys 2021</li>
                      <li> OpenAI. <a href="https://openai.com/blog/gpt-3-apps/"> "GPT-3 Powers the Next Generation of Apps."</a>, OpenAI Blog 2021</li>
                      <li> Eric, Verduzco. <a href="https://nogood.io/2021/06/25/gpt-3-tools/"> "Best GPT-3 Tools, Examples and Use Cases."</a>, 2021</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>                 
                  
                <tr>
                  <td>12</td>
                  <td>5/20</td>
                  <td>AutoML and Practical MLOps <br/> Session 1
                    <td>ÍπÄÏßÄÌõà</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Real, Esteban, et al. <a href="https://arxiv.org/abs/2003.03384"> "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch."</a>, ICML 2020<br/>
                      (2) <span class="label label-primary">response 1</span> Falkner, Stefan, et al. <a href="https://arxiv.org/abs/1807.01774"> "BOHB: Robust and Efficient Hyperparameter Optimization at Scale."</a>, ICML 2018<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Liu, Hanxiao, et al. <a href="https://arxiv.org/abs/1806.09055"> "DARTS: Differentiable Architecture Search."</a>, ICLR 2019</li>
                        <li> Dong, XuanYi and Yang, Yi. <a href="https://arxiv.org/abs/2001.00326"> "NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search."</a>, ICLR 2020</li>
                        Hyperparameter Optimization:
                        <li> Li, Lisha, et al. <a href="https://arxiv.org/abs/1603.06560"> "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization."</a>, JLMR 2018</li>
                        <li> Jaderberg, Max, et al. <a href="https://arxiv.org/abs/1711.09846"> "Population Based Training of Neural Networks."</a>, arXiv 2017</li>
                      </ul>
                    </td>
                  <td></td>
                </tr>
                <tr>
                  <td>12</td>
                  <td>5/20</td>
                  <td>AutoML and Practical MLOps <br/> Session 2
                  <td>ÏÑúÎèôÌïÑ</td>
                  <td>
                    No reading for his session
                  </td>
                  <td></td>
                </tr>
                  
              <tr>
                  <td>13</td>
                  <td>5/27</td>
                  <td>NLP, Dialogues, and QA <br/> Session 1                    </td>
                  <td>Ïù¥ÏÉÅÏö∞</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span> Devlin, et al. <a href="https://arxiv.org/abs/1810.04805"> "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding."</a>, NAACL 2019. 
                    <br/>
                    (2) <span class="label label-primary">response 1</span> Raffel, et al. <a href="https://arxiv.org/abs/1910.10683"> "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer."</a>, JMLR 2020. <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Radford, Alec, et al. <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"> "Language Models are Unsupervised Multitask Learners."</a>, OpenAI 2019</li>
                      <li> Yoo, Kangmin, et al. <a href="https://arxiv.org/abs/2104.08826"> "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation."</a>, ACL Findings 2021</li>
                      <li> Kim, Sungdong, et al. <a href="https://arxiv.org/abs/2105.14454"> "NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation."</a>, , ACL 2021</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>
                <tr>
                  <td>13</td>
                  <td>5/27</td>
                  <td>NLP, Dialogues, and QA <br/> Session 2                    </td>
                  <td>ÍπÄÏÑ±Îèô</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> Roller, Stephen, et al. <a href="https://arxiv.org/abs/2004.13637"> "Recipes for building an open-domain chatbot."</a>, EACL 2021<br/>
                    (2) <span class="label label-primary">response 2</span> Lewis, Patrick, et al. <a href="https://arxiv.org/abs/2005.11401"> "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"</a>, NeurIPS 2020<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Izacard and Grave. <a href="https://arxiv.org/abs/2007.01282"> "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering."</a>, EACL 2021</li>
                      <li> Shuster, Kurt, et al. <a href="https://arxiv.org/abs/2104.07567"> "Retrieval Augmentation Reduces Hallucination in Conversation."</a>, EMNLP Findings 2021</li>
                      <li> Xu, Jing, et al. <a href="https://arxiv.org/abs/2107.07567"> "Beyond Goldfish Memory: Long-Term Open-Domain Conversation."</a>, arXiv 2021</li>
                      <li> Borgeaud, Sebastian, et al. <a href="https://arxiv.org/abs/2112.04426"> "Improving language models by retrieving from trillions of tokens."</a>, arXiv 2021</li>
                      <li> Sungdong, Kim and Gangwoo, Kim. <a href="https://arxiv.org/abs/2202.07280"> "Saving Dense Retriever from Shortcut Dependency in Conversational Search."</a>, arXiv 2022</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>                  
                  
              <tr>
                  <td>14</td>
                  <td>6/3</td>
                  <td>Hyperscale LM & NLP applications <br/> Session 1</td>
                  <td>Ïù¥Í∏∞Ï∞Ω</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span> Brown, et al. <a href="https://arxiv.org/abs/2005.14165"> "Language Models are Few-Shot Learners."</a>, NeurIPS 2021<br/>
                    (2) <span class="label label-primary">response 1</span> Rae, et al. <a href="https://arxiv.org/abs/2112.11446"> "Scaling Language Models: Methods, Analysis & Insights from Training Gopher."</a>, arXiv 2021.  <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Smith, et al. <a href="https://arxiv.org/abs/2201.11990"> "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model."</a>, arXiv 2022</li>
                      <li> Tay, et al., <a href="https://arxiv.org/abs/2109.10686"> "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers."</a>, ICLR 2022</li>
                      <li>Kim, Boseop, et al. <a href="https://arxiv.org/abs/2109.04650"> "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers."</a>, EMNLP 2021</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>
                <tr>
                  <td>14</td>
                  <td>6/3</td>
                  <td>Hyperscale LM & NLP applications <br/> Session 2</td>
                  <td>Ïú†Í∞ïÎØº</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> Lester, Brian, et al. <a href="https://arxiv.org/abs/2104.08691"> "The Power of Scale for Parameter-Efficient Prompt Tuning."</a>, EMNLP 2021<br/>
                    (2) <span class="label label-primary">response 2</span> Li, Xiang Lisa, and Percy, Liang. <a href="https://arxiv.org/abs/2101.00190"> "Prefix-Tuning: Optimizing Continuous Prompts for Generation."</a>, arXiv 2021<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> He, Junxian, et al. <a href="https://arxiv.org/abs/2110.04366"> "Towards a Unified View of Parameter-Efficient Transfer Learning."</a>, ICLR 2022</li>
                      <li> J. Hu, Edward, et al. <a href="https://arxiv.org/abs/2106.09685"> "LoRA: Low-Rank Adaptation of Large Language Models."</a>, arXiv 2021</li>
                      <li> Schick, Timo and Sch√ºtze, Hinrich <a href="https://arxiv.org/abs/2009.07118"> "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners."</a>, NAACL 2021</li>
                      <li> Ouyang, Long, et al. <a href="https://openai.com/blog/instruction-following/"> "Training language models to follow instructions with human feedback (InstructGPT)</a>, OpenAI Blog 2022</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>                  
                  
                <tr>
                  <td>15</td>
                  <td>6/10</td>
                  <td>Human-centric NLP <br/> Session 1</td>
                  <td>Ïù¥ÌôîÎûÄ</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span> Dinan, Emily, et al. <a href="https://arxiv.org/abs/1911.03842"> "Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation."</a>, EMNLP 2020<br/>
                    (2) <span class="label label-primary">response 1</span> Perez, Ethan, et al. <a href="https://arxiv.org/abs/2202.03286"> "Red Teaming Language Models with Language Models."</a>, arXiv 2022.<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Bender, Emily M., et al. <a href="https://dl.acm.org/doi/10.1145/3442188.3445922"> "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ü¶ú."</a>, ACM Conference on Fairness, Accountability, and Transparency 2021</li>
                      <li> Liu, Haochen, et al. <a href="https://arxiv.org/abs/1910.10486"> "Does Gender Matter? Towards Fairness in Dialogue Systems."</a>, COLING 2020</li>
                      <li> Liu, Haochen, et al. <a href="https://arxiv.org/abs/2009.13028"> "Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning."</a>, EMNLP 2020</li>
                      <li> Sheng, Emily, et al. <a href="https://arxiv.org/abs/2010.12820"> "‚ÄúNice Try, Kiddo‚Äù: Investigating Ad Hominems in Dialogue Responses."</a>, NAACL 2021</li>
                      <li> Ma, Xinyao, et al. <a href="https://arxiv.org/abs/2010.13816"> "PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction."</a>, EMNLP 2020</li>
                      <li> Xu, Albert, et al. <a href="https://arxiv.org/abs/2104.06390"> "Detoxifying Language Models Risks Marginalizing Minority Voices."</a>, NAACL 2021</li>
                      <li> OpenAI. <a href=""> "WebGPT: Improving the Factual Accuracy of Language Models through Web Browsing (WebGPT)."</a>, OpenAI Blog 2021</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>

                  <tr>
                    <td>15</td>
                    <td>6/10</td>
                    <td>Human-centric NLP <br/> Session 2</td>
                    <td>Ï†ïÏ§ÄÏòÅ, Ïù¥ÎØºÏïÑ</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Chung, JJY, et al. <a href="https://johnr0.github.io/assets/publications/CHI2022-TaleBrush.pdf">"TaleBrush: Sketching Stories with Generative Pretrained Language Models."</a>, CHI 2022 <br/>
                      (2) <span class="label label-primary">response 2</span> TBD <br/><br/>
                      
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li>Clark, Elizabeth, et al. <a href="https://dl.acm.org/doi/10.1145/3172944.3172983">"Creative writing with a machine in the loop: Case studies on slogans and stories."</a>, IUI 2018</li>
                        <li> Singh, Nikhil, et al. <a href="https://dl.acm.org/doi/10.1145/3511599">"Where to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal Machine Intelligence."</a>, ToCHI 2022</li>
                        <li>Krause, Ben, et al. <a href="https://arxiv.org/abs/2009.06367">"Gedi: Generative discriminator guided sequence generation."</a>, EMNLP findings 2021</li>
                        <li>Qian, Jing et al. <a href="https://arxiv.org/abs/2202.13257">"Controllable Natural Language Generation with Contrastive Prefixes."</a>, arXiv 2022</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>                  
                  
                                    
                  <tr>
                    <td>16</td>
                    <td>6/17</td>
                    <td class="no-class">No invited talk - Student presentations</td>
                    <td class="no-class"></td>
                    <td>TBD</td>
                    <td></td>
                  </tr> 
                                                                                                                                              
                </table>
              </div>
            </div>         
          </div>        
        </div>
    </div>                  

	<div class="row">
    <div class="col-md-4">
          <div class="bs-component">
            <div class="panel panel-primary">
              <div class="panel-heading">
                <h3 class="panel-title">Topics (tentative)</h3>
              </div>
              <div class="panel-body">
                Major topics include:
                <ul>
                 <li>Representation Learning</li>
                 <li>Reliable ML</li>
                 <li>Voice and Speech</li>
                 <li>NLP</li>
                 <li>MLOps</li>
                 <li>Recommendation systems</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Grading</h3>
	            </div>
	            <div class="panel-body">
	            <ul>
                <li>Attendance: 20%</li>
      					<li>Reading responses: 40%</li>
      					<li>Topic presentation: 20%</li>
      					<li>Class participation: 10%</li>
                <li>Quizes: 10%</li>
				      </ul>

      				<strong>Late policy</strong>: Three lowest reading response grades will be removed. No late submissions are allowed for the reading responses.
	            </div>
	          </div>         
	        </div>	        
        </div>
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Prerequisites</h3>
	            </div>
	            <div class="panel-body">
	            There are no official course prerequisites. But assignments involve a lot of reading, research experience in machine learning is useful, but not required. 
	            </div>
	          </div>         
	        </div>	        
        </div>   
    </div>

</div>

<script src="//code.jquery.com/jquery-1.10.2.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="dist/js/ripples.min.js"></script>
<script src="dist/js/material.min.js"></script>
<script>
  $(function () {
    $.material.init();
  });
</script>

</body>
</html>
